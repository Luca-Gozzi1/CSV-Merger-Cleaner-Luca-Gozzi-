"""
Configuration module for Supply Chain Explorer.

This module centralizes all project settings including file paths,
column definitions, model hyperparameters, and business thresholds.
Centralizing configuration improves maintainability and makes it
easy to adjust parameters without modifying core logic.

Author: Luca Gozzi 
Date: November 2025
"""

from pathlib import Path
from typing import Dict, List, Any


# =============================================================================
# PATH CONFIGURATION
# =============================================================================

# Project root directory (parent of src/)
PROJECT_ROOT = Path(__file__).parent.parent

# Data directories
DATA_DIR = PROJECT_ROOT / "data"
RAW_DATA_DIR = DATA_DIR / "raw"
PROCESSED_DATA_DIR = DATA_DIR / "processed"

# Model directory
MODELS_DIR = PROJECT_ROOT / "models"

# Results directory
RESULTS_DIR = PROJECT_ROOT / "results"
FIGURES_DIR = RESULTS_DIR / "figures"

# Raw data file
RAW_DATASET_PATH = RAW_DATA_DIR / "DataCoSupplyChainDataset.csv"

# Processed data files (generated by splitter)
TRAIN_DATA_PATH = PROCESSED_DATA_DIR / "train.csv"
VALIDATION_DATA_PATH = PROCESSED_DATA_DIR / "validation.csv"
TEST_DATA_PATH = PROCESSED_DATA_DIR / "test.csv"

# Model file paths
LOGISTIC_MODEL_PATH = MODELS_DIR / "logistic_regression.pkl"
RANDOM_FOREST_MODEL_PATH = MODELS_DIR / "random_forest.pkl"
XGBOOST_MODEL_PATH = MODELS_DIR / "xgboost.pkl"


# =============================================================================
# DATASET SCHEMA CONFIGURATION
# =============================================================================

# Target variable for ML prediction
TARGET_COLUMN = "Late_delivery_risk"

# Date columns for parsing
DATE_COLUMNS: List[str] = [
    "order date (DateOrders)",
    "shipping date (DateOrders)",
]

# Columns required for ML pipeline (subset of full dataset)
REQUIRED_COLUMNS: List[str] = [
    # Target
    "Late_delivery_risk",
    # Shipping information
    "Days for shipping (real)",
    "Days for shipment (scheduled)",
    "Shipping Mode",
    # Order information
    "Order Item Discount Rate",
    "Order Item Quantity",
    "Order Item Product Price",
    "Sales",
    # Product information
    "Category Name",
    "Product Price",
    # Geographic information
    "Market",
    "Order Region",
    "Order Country",
    "Order City",
    # Customer information
    "Customer Segment",
    # Date columns
    "order date (DateOrders)",
    "shipping date (DateOrders)",
    # Identifiers (for tracking, not features)
    "Order Id",
]

# Columns to use as features (excludes target and identifiers)
FEATURE_COLUMNS: List[str] = [
    "Days for shipping (real)",
    "Days for shipment (scheduled)",
    "Shipping Mode",
    "Order Item Discount Rate",
    "Order Item Quantity",
    "Order Item Product Price",
    "Category Name",
    "Market",
    "Order Region",
    "Customer Segment",
]

# Categorical columns requiring encoding
CATEGORICAL_COLUMNS: List[str] = [
    "Shipping Mode",
    "Category Name",
    "Market",
    "Order Region",
    "Customer Segment",
]

# Numerical columns
NUMERICAL_COLUMNS: List[str] = [
    "Days for shipping (real)",
    "Days for shipment (scheduled)",
    "Order Item Discount Rate",
    "Order Item Quantity",
    "Order Item Product Price",
]

# =============================================================================
# DATA LEAKAGE CONFIGURATION
# =============================================================================

# Features that contain post-delivery information and should NOT be used
# for prediction in a production environment. These features cause data
# leakage because they contain information that would only be available
# AFTER the delivery has occurred.
#
# Using these features results in ~97% accuracy (artificially high).
# Without these features, expect ~70-80% accuracy (realistic).

LEAKY_FEATURES: List[str] = [
    # Raw columns that contain post-delivery information
    "Days for shipping (real)",      # Actual delivery time - only known after delivery
    
    # Engineered features derived from leaky columns
    "shipping_lead_time_variance",   # Uses "Days for shipping (real)"
    "shipping_time_ratio",           # Uses "Days for shipping (real)"
    "order_processing_time",         # Uses "shipping date" which may not be known at order time
    
    # Status columns that indicate delivery outcome
    "Delivery Status",               # Directly encodes whether delivered
    "Order Status",                  # May indicate delivery status
]

# Flag to control whether to remove leaky features
# Set to True for realistic production-like evaluation
# Set to False to see maximum possible accuracy (with data leakage)
REMOVE_LEAKY_FEATURES: bool = True

# =============================================================================
# DATA SPLITTING CONFIGURATION
# =============================================================================

# Time-based split ratios
# Using time-based split to prevent data leakage from future observations
TRAIN_RATIO = 0.70  # 70% for training
VALIDATION_RATIO = 0.15  # 15% for validation/hyperparameter tuning
TEST_RATIO = 0.15  # 15% for final evaluation

# Random seed for reproducibility
RANDOM_SEED = 42


# =============================================================================
# OPTIMIZED MODEL HYPERPARAMETERS
# =============================================================================

LOGISTIC_REGRESSION_PARAMS: Dict[str, Any] = {
    "C": 0.5,
    "max_iter": 2000,
    "random_state": RANDOM_SEED,
    "solver": "saga",
    "class_weight": "balanced",
}

RANDOM_FOREST_PARAMS: Dict[str, Any] = {
    "n_estimators": 200,
    "max_depth": 20,
    "min_samples_split": 5,
    "min_samples_leaf": 2,
    "max_features": "sqrt",
    "random_state": RANDOM_SEED,
    "class_weight": "balanced_subsample",
    "n_jobs": -1,
}

XGBOOST_PARAMS: Dict[str, Any] = {
    "n_estimators": 200,
    "max_depth": 8,
    "learning_rate": 0.05,
    "subsample": 0.8,
    "colsample_bytree": 0.8,
    "min_child_weight": 3,
    "gamma": 0.1,
    "random_state": RANDOM_SEED,
    "eval_metric": "logloss",
}

# =============================================================================
# BUSINESS THRESHOLDS
# =============================================================================

# Risk classification thresholds for delay probability
# These convert continuous probabilities to business-meaningful categories
RISK_THRESHOLDS: Dict[str, float] = {
    "low": 0.3,  # Below 30% = Low risk
    "medium": 0.6,  # 30-60% = Medium risk
    "high": 0.6,  # Above 60% = High risk  # Above 60% = High risk
}

# Alert priority levels based on risk
ALERT_PRIORITIES: Dict[str, int] = {
    "High": 1,  # Immediate attention
    "Medium": 2,  # Monitor closely
    "Low": 3,  # Standard processing
}


# =============================================================================
# EVALUATION METRICS CONFIGURATION
# =============================================================================

# Primary metric for model selection
PRIMARY_METRIC = "recall"  # Prioritize catching late deliveries

# All metrics to calculate
EVALUATION_METRICS: List[str] = [
    "accuracy",
    "precision",
    "recall",
    "f1",
    "roc_auc",
]

# Minimum acceptable performance thresholds
MIN_ACCURACY = 0.80
MIN_RECALL = 0.75
MIN_ROC_AUC = 0.80


# =============================================================================
# VISUALIZATION CONFIGURATION
# =============================================================================

# Figure size defaults
FIGURE_SIZE_SMALL = (8, 6)
FIGURE_SIZE_MEDIUM = (10, 8)
FIGURE_SIZE_LARGE = (14, 10)

# Color palette for consistency
COLORS: Dict[str, str] = {
    "primary": "#2E86AB",  # Blue
    "secondary": "#A23B72",  # Purple
    "success": "#18A558",  # Green
    "warning": "#F39C12",  # Orange
    "danger": "#E74C3C",  # Red
    "neutral": "#95A5A6",  # Gray
}


# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================

LOG_FORMAT = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
LOG_DATE_FORMAT = "%Y-%m-%d %H:%M:%S"
LOG_LEVEL = "INFO"